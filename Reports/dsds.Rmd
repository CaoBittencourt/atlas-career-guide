---
title: "Career Finder Report"
author: "Atlas Research Team"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: TRUE
    number_sections: TRUE
    toc_depth: 4
header-includes:
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)

pkg <- c(
  'FNN' #Fast K-NN Algorithm (faster than the 'class' package)
  , 'jsonify' #Work with JSON (faster than jsonlite)
  , 'ggthemes' #Data visualization
  , 'tidyverse', 'glue' #Data wrangling
  , 'tinytex' #LaTeX
)

# Activate / install packages
lapply(pkg, function(x)
  if(!require(x, character.only = T))
  {install.packages(x); require(x)})

# Install TinyTex
if(!tinytex::is_tinytex()){
  tinytex::install_tinytex()
}

# Increase buffer size
# Sys.setenv("VROOM_CONNECTION_SIZE" = 10000000)

# Package citation
# lapply(pkg, function(x)
#   {citation(package = x)})
```

```{r KNN.matching_function}

fun_KNN.matching <- function(
    .df_data.numeric
    , .vec_query.numeric
    , .int_k = 1
    , .auto_select.k = F
    , .imput.over_qualification = T
    , .dbl_over_qualification.threshold = 0.1
    , .dbl_decimals = 4
){

  # Get numeric data only
  .df_data.numeric %>%
    select(where(is.numeric)) -> .df_data.numeric.temp

  if(is.data.frame(.vec_query.numeric)){
    .vec_query.numeric %>%
      select(where(is.numeric)) -> .vec_query.numeric
  }

  # Define k
  if(.auto_select.k){
    # RECOMMENDED
    # Typical suggested value for k is sqrt(nrow(df))
    # Looking for k nearest neighbors in all career clusters

    .df_data.numeric %>%
      nrow(.) %>%
      sqrt(.) %>%
      round(.) -> .int_k

  }

  if(.imput.over_qualification){

    .vec_query.numeric %>%
      rename_with(
        .fn = function(x){paste0(x,'.imput')}
      ) %>%
      bind_cols(
        .df_data.numeric.temp
      ) %>%
      mutate(
        across(
          .cols = c(
            !ends_with('.imput')
          )
          ,.fns = function(x){

            ifelse(
              # Overqualified if > cutoff and requirement <= cutoff
              x <= .dbl_over_qualification.threshold
              & eval(sym(paste0(cur_column(),'.imput'))) > x
              , yes = x
              , no = eval(sym(paste0(cur_column(),'.imput')))
            )

          }
          , .names = '{col}.sub'
        )
      ) %>%
      select(
        ends_with('.sub')
      ) %>%
      rename_with(
        function(x){str_remove(x,'.sub')}
      ) -> .vec_query.numeric

    lapply(
      1:nrow(.vec_query.numeric)
      , function(x){

        FNN::get.knnx(
          data = .df_data.numeric.temp[x,]
          , query = .vec_query.numeric[x,]
          , k = 1
        ) -> KNN.output

      }) %>%
      bind_rows() -> KNN.output

    # # Find the k nearest neighbors
    # FNN::get.knnx(
    #   data = .df_data.numeric.temp
    #   , query = .vec_query.numeric
    #   , k = 1
    # ) -> KNN.output

    # KNN.output$nn.index[,1] -> KNN.output$nn.index
    #
    # KNN.output$nn.dist[,1] -> KNN.output$nn.dist

    # Arrange original data frame with KNN output
    .df_data.numeric %>%
      mutate(#Add euclidean distances and convert them to similarities
        Euclidean_Distance = as.vector(KNN.output$nn.dist)

        , Similarity = 1 - (pmin(Euclidean_Distance,2.5) ^ 2) / 3.125 #Bound at [-1,1]

        , across(
          .cols = starts_with('Similarity.')
          ,.fns = function(x){round(x,.dbl_decimals)}
        )

      ) %>%
      arrange(Euclidean_Distance) %>%
      return(.)

  } else {

    # Find the k nearest neighbors
    FNN::get.knnx(
      data = .df_data.numeric.temp
      , query = .vec_query.numeric
      , k = .int_k
    ) -> KNN.output

    # Arrange original data frame with KNN output
    .df_data.numeric %>%
      slice(as.vector(KNN.output$nn.index)) %>%
      mutate(#Add euclidean distances and convert them to similarities
        Euclidean_Distance = as.vector(KNN.output$nn.dist)

        , Similarity = 1 - (pmin(Euclidean_Distance,2.5) ^ 2) / 3.125 #Bound at [-1,1]

        , across(
          .cols = starts_with('Similarity.')
          ,.fns = function(x){round(x,.dbl_decimals)}
        )

      ) %>%
      arrange(Euclidean_Distance) %>%
      return(.)

  }

}

```

```{r skills.list}

list_skill.factors <- list(
  
  'Discernment' = c(
    # Factor 1 is composed of cognitive, non-technical, general competencies related to decision-making (discernment)
    'judgment_and_decision.l'
    , 'active_learning.l'
    , 'complex_problem_solving.l'
    , 'critical_thinking.l'
  )
  , 'Technical Skills' = c(
    # Factor 2 is composed of mechanical, hands-on, specialist skills (technical)
    'equipment_selection.l'
    , 'troubleshooting.l'
    , 'repairing.l'
    , 'equipment_maintenance.l'
  )
  
)

```

```{r abilities.list}

list_ablt.factors <- list(
  
  'Perception' = c(
    # Factor 1 is composed of perceptual abilities (perception):
    'glare_sensitivity.l'
    , 'peripheral_vision.l'
    , 'sound_localization.l'
  )
  , 'Dexterity' = c(
    # Factor 2 is composed of manual abilities (dexterity):
    'arm_hand_steadiness.l'
    , 'finger_dexterity.l'
    , 'control_precision.l'
  )
  , 'Intelligence' = c(
    # Factor 4 is composed of cognitive abilities (intelligence):
    'inductive_reasoning.l'
    , 'deductive_reasoning.l'
    , 'information_ordering.l'
  )
  
)

```

```{r knowledge.list}

list_know.factors <- list(
  
  'Health Science' = c(
    # Factor 1 is composed of health-related fields of knowledge (health).
    'therapy_and_counseling.l'
    , 'medicine_and_dentistry.l'
    , 'psychology.l'
    , 'biology.l'
  )
  , 'Building' = c(
    # Factor 2 is composed of engineering / building-related fields of knowledge (building).
    'physics.l'
    , 'engineering_and_technology.l'
    , 'mechanical.l'
    , 'building_and_construction.l'
  ) 
  , 'Business' = c(
    # Factor 3 is composed of financial and enterprising fields of knowledge (business).
    'administration_and_management.l'
    , 'economics_and_accounting.l'
    , 'personnel_and_human_resources.l'
    , 'administrative.l'
  )
  , 'Arts & Humanities' = c(
    # Factor 4 is composed of arts and humanities (arts & humanities).
    'history_and_archeology.l'
    , 'fine_arts.l'
    , 'philosophy_and_theology.l'
    , 'sociology_and_anthropology.l'
  ) 
  
)

```

```{r factors.list}

list_factors <- list(
  'Skills' = list_skill.factors
  , 'Abilities' = list_ablt.factors
  , 'Fields of Knowledge' = list_know.factors
)

```

```{r occupations.data, include=FALSE}

# Occupations data frame
df_occupations.all <- readr::read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSphzWoCxoNaiaJcQUWKCMqUAT041Q8UqUgM7rSzIwYZb7FhttKJwNgtrFf-r7EgzXHFom4UjLl2ltk/pub?gid=563902602&single=true&output=csv')

# Select only necessary variables
df_occupations.all %>%
  select(
    occupation
    , career_cluster
    , entry_level_education
    , all_of(
      list_factors %>%
        flatten() %>% 
        flatten_chr()
    )
  ) %>%
  mutate(
    across(
      .cols = all_of(
        list_factors %>%
          flatten() %>% 
          flatten_chr()
      )
      , .fns = function(x){x/100}
    )
  ) -> df_occupations

```

```{r questionnaire.data, include=FALSE}

# User questionnaires data frame
df_input <- readr::read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSphzWoCxoNaiaJcQUWKCMqUAT041Q8UqUgM7rSzIwYZb7FhttKJwNgtrFf-r7EgzXHFom4UjLl2ltk/pub?gid=725827850&single=true&output=csv')

# Selected respondent
chr_user <- 'Martijn'
# chr_user <- 'Gabriel'

df_input %>% 
  filter(Name == chr_user) %>%
  select(-Name) -> df_input

df_input %>%
  select(
    all_of(
      list_factors %>%
        flatten() %>%
        flatten_chr()
    )
  ) %>%
  mutate(
    across(
      .cols = all_of(
        list_factors %>%
          flatten() %>%
          flatten_chr()
      )
      , .fns = function(x){
        recode(x
               , '1' = .0
               , '2' = .25
               , '3' = .5
               , '4' = .75
               , '5' = 1
        )}
    )
  ) -> df_input

```

```{r KNN.matching, include=FALSE}

# OVERQUALIFICATION IMPUTATION --------------------------------------------
# OVERQUALIFICATION IMPUTATION - 100% UNNECESSARY COMPETENCY (0)
# OVERQUALIFICATION IMPUTATION - 5-POINT LIKERT LOW / 4 (0.0625)
# OVERQUALIFICATION IMPUTATION - 5-POINT LIKERT LOW / 2 (0.125)
# OVERQUALIFICATION IMPUTATION - 5-POINT LIKERT LOW (0.25)
# OVERQUALIFICATION IMPUTATION - 10-POINT LIKERT VERY LOW / 2 (0.05)
# OVERQUALIFICATION IMPUTATION - 10-POINT LIKERT VERY LOW (0.10)
# OVERQUALIFICATION IMPUTATION - 10-POINT LIKERT LOW (0.20)
dbl_threshold <- 0

fun_KNN.matching(
  .df_data.numeric = df_occupations
  , .vec_query.numeric = df_input
  , .int_k = nrow(df_occupations)
  , .imput.over_qualification = T
  , .dbl_over_qualification.threshold = dbl_threshold
  , .dbl_decimals = 4
) -> df_KNN.output

```

\newpage

# Introduction

This is your own personalized career matching report!Â 

# Methodology

## Data

In order to construct and test our psychometric models and questionnaires, we've gathered publicly available data from the *Bureau of Labor Statistics*[^1]. In addition to the BLS official website, this data can also be found neatly organized at *O'NET Online*[^2].

[^1]: See <https://www.bls.gov/data/home.htm>.

[^2]: See <https://www.onetonline.org/>.

```{r data.text, include=FALSE}

df_occupations.all %>% 
  select(ends_with('.l')) %>% 
  ncol() -> int_ncol

df_occupations.all %>% 
  nrow() -> int_nrow

df_occupations.all %>% 
  pull(career_cluster) %>%
  unique() -> chr_clusters

chr_clusters %>% 
  length() -> int_clusters

chr_clusters %>%
  sample(2) -> chr_sample1

chr_clusters[!(chr_clusters %in% chr_sample1)] %>%
  sample(1) -> chr_sample2

```

Our data base consists in a set of `r int_ncol` job characteristics, such as entry level of education, required skills, abilities and other competencies (rated from 0 to 1), as well as typical job activities, job hazards, and so on.

This information is currently available for `r int_nrow` occupations, spread across `r int_clusters` different career clusters, including `r chr_sample1`, `r chr_sample2`, and more.

## Matching

One of our goals at Atlas Research^Â©^ is to find the occupation that best fits you. Therefore, we assess your most important competencies and job preferences, and compare these with all `r int_nrow` occupations in our data base. Then, we arrange your matches best to worst and estimate a compatibility score ranging from 1 to -1 (i.e. from 100% compatibility to 100% incompatibility).

To do so, we utilize the *K-Nearest Neighbors algorithm* (KNN). This fast and effective machine learning method compares a vector to a given matrix by successively measuring the Euclidean distances between them, which are defined as:

$$
d(p,q) = \sqrt{\sum_{i=1}^{n}{(p_i-q_i)^2}},
$$

where $p$ is a vector with $n$ components and $q$, one line of a comparison matrix. The algorithm then proceeds by sorting these distances from smallest (greatest similarity) to largest (lowest similarity), and returns the *k* smallest distances (i.e. the *k* "nearest neighbors").

Thus, after assessing your competencies and job preferences, we apply the KNN algorithm to your questionnaire results and produce a detailed list of careers that approximate your profile.

## Factor Analysis

```{r questionnaire.text, include=FALSE}
df_input %>% ncol() -> int_quiz.length
```

Now, to adequately capture your professional profile, we've developed a highly internally consistent psychometric questionnaire. And, in addition to being based on a reliable construct, this instrument is relatively short: instead of plowing through an enormous `r int_ncol`-item survey, you can find your best career matches by answering only `r int_quiz.length` very simple questions! But how did we accomplish this?

The answer is: factor analysis. This psychometric procedure aims to explain relationships between different *observed* variables in terms of latent, *unobserved* variables, or factors. These factors, then, function as "groups" in which observed variables cluster together, according to how they correlate to one another. After we find the underlying factors, we analyze how strongly each variable is associated to them. We then reduce dimensionality by keeping a fraction of the original data base, that is: those variables which best represent each factor (the "purest" variables). Therefore, with a much, much smaller set of questionnaire items, we can assertively assess your professional profile and match you to your best possible career!

The details of our factor analysis procedures can be found in the appendix at end of this report.

\newpage

# Results

## Top 5 Career Matches

Dear, `r chr_user`, these are the 5 careers that best approximate your competencies and job preferences:

```{r caption1, include=FALSE}

glue('Top 5 Matches â {chr_user}') -> chr_caption1

```

```{r top5matches.table}
df_KNN.output %>% 
  select(
    occupation
    , career_cluster
    , Similarity
  ) %>% 
  rename(
    Occupation = occupation
    , Cluster = career_cluster
    , Compatibility = Similarity
  ) %>% 
  head(5) %>% 
  # mutate(
  #   across(
  #     .cols = where(is.character)
  #     ,.fns = function(x){str_wrap(x,22)}
  #   )
  # ) %>%
  mutate(
    Compatibility = scales::percent(Compatibility, accuracy = .01)
  ) %>% 
  knitr::kable(
    caption = chr_caption1
    # , align = 'c'
    # , escape = F
  )
```

We can analyze them graphically like so:

```{r top5matches.plot, message=FALSE, fig.cap=chr_caption1}

df_KNN.output %>%
  arrange(desc(Similarity)) %>% 
  head(5) %>% 
  mutate(
    occupation = fct_reorder(
      .f = occupation
      , .x = Similarity
      , .fun = max
      , .desc = F
    )
    , career_cluster = factor(career_cluster)
  ) %>%
  ggplot(aes(
    x = Similarity
    , y = occupation
    , fill = career_cluster
    , label = scales::percent(Similarity, accuracy = .01)
  )) +
  geom_col() + 
  geom_text(aes(color = career_cluster)
    , position = position_dodge(width = 0.9)
    , hjust = -0.13
    , size = 3
    ) + 
  guides(
    fill = guide_legend(title.position = 'top')
    , color = 'none'
  ) +
  labs(
    x = 'Similarity (%)'
    , y = NULL
    , title = NULL
    , fill = 'Career Cluster'
  ) +
  scale_x_continuous(
    limits = c(0,1)
    , labels = scales::percent_format()
    ) +  
  scale_y_discrete(
    labels = function(x){str_wrap(x, width = 22)} 
  ) + 
  ggthemes::scale_fill_gdocs() + 
  ggthemes::scale_color_gdocs() + 
  ggthemes::theme_tufte() + 
  theme(
    legend.position = 'bottom'
    # , legend.direction = 'vertical'
    , legend.title.align = 0.5
  )
```

\newpage

## Factor Scores

We can also summarize your professional profile calculating factor scores. These measure how much you average at each of the factors in the questionnaire[^3]. For instance, considering your competencies, we have:

[^3]: See [Factor Analysis] above.

```{r factor.names}

list_factors %>% 
  bind_rows(
    .id = 'competency'
    ) %>% 
  pivot_longer(
    cols = -competency
    ) %>% 
  drop_na() %>% 
  group_by(competency) %>% 
  summarise(
    name = unique(name)
    ) -> df_factors.names

```

```{r factor.scores, include=FALSE}

lapply(
  list_factors
  , function(scales){
    
    psych::scoreVeryFast(
      keys = scales
      , items = df_input 
      , totals = F #Average scores
    ) %>% 
      as_tibble() %>% 
      colMeans()
    
  } 
) %>% 
  flatten_df() -> df_factor.scores

df_factor.scores %>% 
  pivot_longer(
    cols = everything()
  ) %>% 
  full_join(
    df_factors.names
  ) -> df_factor.scores

```

```{r caption2, include=FALSE}

glue('Factor Scores â {chr_user}') -> chr_caption2

```

```{r scores.plot, fig.cap=chr_caption2}

df_factor.scores %>% 
  mutate(
    name = fct_reorder(name, value)
  ) %>% 
  ggplot(aes(
    x = name
    , y = value
    , fill = competency
    , label = scales::percent(value, accuracy = .01)
  )) + 
  geom_col() +
  geom_text(aes(color = competency)
    , position = position_dodge(width = 0.9)
    , vjust = -0.25
    , size = 3
    ) + 
  guides(
    fill = guide_legend(title.position = 'top')
    , color = 'none'
  ) + 
  labs(
    x = 'Factor'
    , y = 'Factor Score'
    , title = NULL
    , fill = 'Competency'
  ) + 
  scale_x_discrete(
    labels = function(x){str_wrap(x,10)}
  ) + 
  scale_y_continuous(
    limits = c(0,1)
    , labels = scales::percent_format()
    ) + 
  ggthemes::scale_fill_gdocs() + 
  ggthemes::scale_color_gdocs() + 
  ggthemes::theme_tufte() + 
  theme(
    legend.position = 'bottom'
    # , legend.direction = 'vertical'
    , legend.title.align = 0.5
  )

```
