map_dbl(
.x = df_occupations.t[-1]
, ~
as.numeric(
1 +
weights::wtd.cors(
df_occupations.t[1]
, .x
, weight = .x
)
) / 2
) -> df_models$pearson.wgt
date_end <- Sys.time()
df_models.time$pearson.wgt <- date_end - date_start
# - Weighted BVLS matching -----------------------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = df_occupations.t[-1]
, ~
bvls(
as.matrix(.x * sqrt(.x))
, df_occupations.t[[1]] *
sqrt(.x)
, bl = dbl_similarity.lb
, bu = dbl_similarity.ub
)[[1]]
) -> df_models$bvls.wgt
date_end <- Sys.time()
df_models.time$bvls.wgt <- date_end - date_start
# - Weighted Logistic regression matching -----------------------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = df_occupations.t[-1] / 100
, ~
fastglm(
as.matrix(.x)
, df_occupations.t[[1]] / 100
, family = binomial(
link = 'logit'
)
, weights = as.matrix(.x)
) %>%
coef()
) -> df_models$logit.wgt
exp(df_models$logit.wgt) /
(1 + exp(df_models$logit.wgt)) ->
df_models$logit.wgt
date_end <- Sys.time()
df_models.time$logit.wgt <- date_end - date_start
# - Weighted Probit regression matching -----------------------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = df_occupations.t[-1] / 100
, ~
fastglm(
as.matrix(.x)
, df_occupations.t[[1]] / 100
, family = binomial(
link = 'probit'
)
, weights = as.matrix(.x)
) %>%
coef()
) -> df_models$probit.wgt
exp(df_models$probit.wgt) /
(1 + exp(df_models$probit.wgt)) ->
df_models$probit.wgt
date_end <- Sys.time()
df_models.time$probit.wgt <- date_end - date_start
# - Weighted Beta regression matching ----------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = 2:ncol(df_occupations.t)
, ~
gamlss::gamlss(
V1 ~ 0 + .
, sigma.formula = ~ 0 + .
, nu.formula = ~ 0 + .
, tau.formula = ~ 0 + .
, family = BEINF
, data = df_occupations.t[c(1,,x)] / 100
, weights = df_occupations.t[[.x]]
) %>%
coef() %>%
pmin(1) %>%
pmax(0)
) -> df_models$beta
date_end <- Sys.time()
df_models.time$beta <- date_end - date_start
# betareg::betareg.fit(
#   x = as.matrix(df_occupations.t[[10]] / sum(df_occupations.t[[10]]))
#   , y = df_occupations.t[[1]] / sum(df_occupations.t[[1]])
#   # , z = as.matrix(df_occupations.t[[10]] / 100)
# )
# betareg::betareg(
#   formula = V1 ~ 0 + .
#   , data =
#     df_occupations.t %>%
#     select(1,10) %>%
#     mutate(across(
#       .cols = everything()
#       ,.fns = ~
#         pmin(.x, 99.9999999999) %>%
#         pmax(0.0000000001) / 100
#     ))
# ) %>% coef()
# - Weighted Beta regression matching ----------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = 2:ncol(df_occupations.t)
, ~
gamlss::gamlss(
V1 ~ 0 + .
, sigma.formula = ~ 0 + .
, nu.formula = ~ 0 + .
, tau.formula = ~ 0 + .
, family = BEINF
, data = df_occupations.t[c(1,.x)] / 100
, weights = df_occupations.t[[.x]]
) %>%
coef() %>%
pmin(1) %>%
pmax(0)
) -> df_models$beta
date_end <- Sys.time()
df_models.time$beta <- date_end - date_start
# betareg::betareg.fit(
#   x = as.matrix(df_occupations.t[[10]] / sum(df_occupations.t[[10]]))
#   , y = df_occupations.t[[1]] / sum(df_occupations.t[[1]])
#   # , z = as.matrix(df_occupations.t[[10]] / 100)
# )
# betareg::betareg(
#   formula = V1 ~ 0 + .
#   , data =
#     df_occupations.t %>%
#     select(1,10) %>%
#     mutate(across(
#       .cols = everything()
#       ,.fns = ~
#         pmin(.x, 99.9999999999) %>%
#         pmax(0.0000000001) / 100
#     ))
# ) %>% coef()
# - Pivot models ------------------------------------------------------
df_models %>%
pivot_longer(
cols = -1
, names_to = 'model'
, values_to = 'similarity'
) -> df_models.long
# - Running time ----------------------------------------------------------
df_models.time %>%
pivot_longer(
cols = everything()
, names_to = 'model'
, values_to = 'time'
) %>%
mutate(
time = as.numeric(time)
, time.fct =
cut(
time
, breaks = 5
, labels = F
) %>%
recode(
'5' = 'Very Slow'
, '4' = 'Slow'
, '3' = 'Medium'
, '2' = 'Fast'
, '1' = 'Very Fast'
) %>%
factor(
levels =
c(
'Very Slow'
, 'Slow'
, 'Medium'
, 'Fast'
, 'Very Fast'
)
)
) %>%
fun_plot.lollipop(aes(
x = model
, y = time
, color = time.fct
)
, .list_labs =
list(
x = NULL
, y = 'Running Time (s)'
, title = 'Comparison of Different Matching Models'
, subtitle = 'Running Time for Estimating Each Model'
, color = NULL
)
, .chr_manual.pal =
set_names(
viridis::viridis(5)
, c(
'Very Slow'
, 'Slow'
, 'Medium'
, 'Fast'
, 'Very Fast'
)
)
, .list_axis.y.args =
list(
limits = c(0,1.2*max(as.numeric(df_models.time)))
, breaks = seq(0,1.2*max(as.numeric(df_models.time)), length.out = 10)
)
) +
annotate(
x = ceiling(ncol(df_models.time) / 3)
, y = 0.9 * max(as.numeric(df_models.time))
, geom = 'text'
, label =
str_wrap(
'The old models, which are still in use, are much slower compared to the weighted knn revised algorithm, as well as all regression approches.
Concerning these newer matching models, we can see they generally run in less than 1s.
It is also evident that logit and probit models are the slowest among the regression methodologies.'
, width = 50
)
)
# - Weighted Beta regression matching ----------------------------------------------
date_start <- Sys.time()
map_dbl(
.x = 2:ncol(df_occupations.t)
, ~
gamlss::gamlss(
V1 ~ 0 + .
, sigma.formula = ~ 0 + .
, nu.formula = ~ 0 + .
, tau.formula = ~ 0 + .
, family = BEINF
, data = df_occupations.t[c(1,.x)] / 100
, weights = df_occupations.t[[.x]]
) %>%
coef() %>%
pmin(1) %>%
pmax(0)
) -> df_models$beta.wgt
date_end <- Sys.time()
df_models.time$beta.wgt <- date_end - date_start
# betareg::betareg.fit(
#   x = as.matrix(df_occupations.t[[10]] / sum(df_occupations.t[[10]]))
#   , y = df_occupations.t[[1]] / sum(df_occupations.t[[1]])
#   # , z = as.matrix(df_occupations.t[[10]] / 100)
# )
# betareg::betareg(
#   formula = V1 ~ 0 + .
#   , data =
#     df_occupations.t %>%
#     select(1,10) %>%
#     mutate(across(
#       .cols = everything()
#       ,.fns = ~
#         pmin(.x, 99.9999999999) %>%
#         pmax(0.0000000001) / 100
#     ))
# ) %>% coef()
# - Pivot models ------------------------------------------------------
df_models %>%
pivot_longer(
cols = -1
, names_to = 'model'
, values_to = 'similarity'
) -> df_models.long
# - Running time ----------------------------------------------------------
df_models.time %>%
pivot_longer(
cols = everything()
, names_to = 'model'
, values_to = 'time'
) %>%
mutate(
time = as.numeric(time)
, time.fct =
cut(
time
, breaks = 5
, labels = F
) %>%
recode(
'5' = 'Very Slow'
, '4' = 'Slow'
, '3' = 'Medium'
, '2' = 'Fast'
, '1' = 'Very Fast'
) %>%
factor(
levels =
c(
'Very Slow'
, 'Slow'
, 'Medium'
, 'Fast'
, 'Very Fast'
)
)
) %>%
fun_plot.lollipop(aes(
x = model
, y = time
, color = time.fct
)
, .list_labs =
list(
x = NULL
, y = 'Running Time (s)'
, title = 'Comparison of Different Matching Models'
, subtitle = 'Running Time for Estimating Each Model'
, color = NULL
)
, .chr_manual.pal =
set_names(
viridis::viridis(5)
, c(
'Very Slow'
, 'Slow'
, 'Medium'
, 'Fast'
, 'Very Fast'
)
)
, .list_axis.y.args =
list(
limits = c(0,1.2*max(as.numeric(df_models.time)))
, breaks = seq(0,1.2*max(as.numeric(df_models.time)), length.out = 10)
)
) +
annotate(
x = ceiling(ncol(df_models.time) / 3)
, y = 0.9 * max(as.numeric(df_models.time))
, geom = 'text'
, label =
str_wrap(
'The old models, which are still in use, are much slower compared to the weighted knn revised algorithm, as well as all regression approches.
Concerning these newer matching models, we can see they generally run in less than 1s.
It is also evident that logit and probit models are the slowest among the regression methodologies.'
, width = 50
)
)
# - Similarity distribution -----------------------------------------------
df_models.long %>%
group_by(model) %>%
mutate(
similarity.kflex =
fun_kflex(similarity)
) %>%
ungroup() %>%
mutate(
similarity.kflex =
cut(
similarity.kflex
, breaks = 5
, labels = F
) %>%
recode(
'5' = 'Very Generalist'
, '4' = 'Generalist'
, '3' = 'Normal'
, '2' = 'Specialist'
, '1' = 'Very Specialist'
) %>%
factor(
levels =
c(
'Very Generalist'
, 'Generalist'
, 'Normal'
, 'Specialist'
, 'Very Specialist'
)
)
) %>%
ungroup() %>%
fun_plot.ridges(aes(
x = similarity
, y = model
, fill = similarity.kflex
)
, .list_labs =
list(
title = 'Similarity Distribution for Each Model'
, subtitle = 'Note: These similarity coefficients are not representative of the whole population.'
, x = 'Professional Compatibility Scores (%)'
, y = NULL
, fill = NULL
)
, .list_legend = list(fill = guide_legend(nrow = 1))
, .chr_manual.pal =
set_names(
viridis::viridis(5)
,c(
'Very Generalist'
, 'Generalist'
, 'Normal'
, 'Specialist'
, 'Very Specialist'
)
)
, .list_axis.x.args =
list(
limits = c(0,1.1)
, breaks = seq(0,1,length.out = 7)
)
, .list_axis.y.args =
list(
expand = c(0.25,0)
)
, .fun_format.x = percent
)
# - Interpretability ------------------------------------------------------
map(
set_names(
names(df_models)[-1]
, names(df_models)[-1]
)
, ~
df_models %>%
arrange(desc(
!!sym(.x)
)) %>%
select(
1, !!sym(.x)
)
) -> list_models
list_models
list_models$beta
list_models$beta %>% print(n = 30)
list_models$beta.wgt %>% print(n = 30)
list_models$beta.wgt %>% tail(100)
Rborist::rfArb(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
) -> dsds
dsds
coef(dsds)
coefficients(dsds)
fitted(dsds)
dsds$prediction
dsds$prediction$yPred
df_occupations.t[[10]]
dsds$prediction$yPred
dsds$prediction$yPred /
df_occupations.t[[10]]
solve(
df_occupations.t[[10]]
, dsds$prediction$yPred
)
solve(
dsds$prediction$yPred
, df_occupations.t[[10]]
)
dsds$forest
rpart::rpart(
speed ~ .
, data = cars
, weights = dist
) -> dsds
dsds
summary(dsds)
coef(dsds)
coefficients(dsds)
install.packages('randomForest')
library(randomForest)
library(randomForest)
importance(dsds)
?randomForest(
)
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
)
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
, weights = df_occupations.t[[10]]
)
df_occupations.t[[10]]
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
, weights = df_occupations.t[[10]]+0.0001
)
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
# , weights = df_occupations.t[[10]]+0.0001
) %>%
importance()
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
, weights = df_occupations.t[[10]]+0.0001
) %>%
importance()
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
, weights = df_occupations.t[[10]]+0.0001
, importance = T
)
randomForest(
x = as.matrix(df_occupations.t[[10]])
, y = df_occupations.t[[1]]
, weights = df_occupations.t[[10]]+0.0001
, importance = T
) -> dsds
dsds$importance
