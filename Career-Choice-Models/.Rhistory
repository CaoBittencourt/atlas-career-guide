geom_tile()
# Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading.Diff.Abs
)) +
geom_tile() +
scale_fill_gradient2(
low = "#FF0000"
, mid = "#FFFFCC"
, high = "#075AFF")
# Significant Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Diff.Significant
)) +
geom_tile()
# Internal consistency test: Cronbach's Alpha
# Do the factors form a coherent group in and of themselves?
# Matching items to factors
# df_manually_assign <- read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSphzWoCxoNaiaJcQUWKCMqUAT041Q8UqUgM7rSzIwYZb7FhttKJwNgtrFf-r7EgzXHFom4UjLl2ltk/pub?gid=26093558&single=true&output=csv')
df_loadings.long %>%
right_join(
df_manually_assign
, by = c('Metric' = 'Subspecies')
) -> df_loadings.long
df_loadings.long %>%
filter(
Factor == Classification5_Factor
) -> df_loadings.long.factors
# Separate factors into individual data frames
lapply(
str_sort(unique(df_loadings.long.factors$Factor))
, function(factors){
df_loadings.long.factors %>%
filter(
Factor == factors
) %>%
pull(Metric) %>%
factor(.) %>%
return(.)
}) -> list_chr_loadings.long.factors
# Apply Alpha test to each subset of variables
lapply(
list_chr_loadings.long.factors
, function(factors){
df_occupations.numeric %>%
select(factors) -> df.temp #Select only the variables that match to each factor
if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
df.temp %>%
alpha(.) %>%
return(.)
}
else{
return(NA)
}
}) -> list_alpha
# Raw alpha score
lapply(
seq_along(list_alpha)
, function(index){
if(!is.na(list_alpha[index])){
list_alpha[[index]]$total$raw_alpha %>%
return(.)
}
else{
return(NA)
}
}) -> list_raw_alpha
# The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
lapply(
list_raw_alpha
, function(cronbach){
cronbach %>%
as_tibble(.) -> cronbach
colnames(cronbach) <- 'Cronbach_Alpha'
cronbach %>%
mutate(
Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
) %>%
return(.)
}) %>%
bind_rows(.) %>%
mutate(
Factor = paste0('Factor', row_number())
) -> df_raw_alpha
df_raw_alpha
# Conclusion:
# Rationals, Guardians, and Idealists (Factors 1, 3 and 4) are internally consistent
# Guardians (Factor 3) is only slightly above the minimum threshold for consistency
# Artisans (Factor 2) are not consistent
# Factor 1 has 9 fields of knowledge loading to it.
# Factor 2 has 9 fields of knowledge loading to it.
# Factor 3 has 5 fields of knowledge loading to it.
# Factor 3 has 10 fields of knowledge loading to it.
df_loadings.long.factors %>%
group_by(Factor) %>%
tally(.) %>%
mutate(prct = round(n/sum(n),2))
# Manual EFA VI (Holland): 6 factors (R, I, A, S, E, C) --------------------------------------------------------
# # Exploratory Factor Analysis (EFA)
# 'Not in' operator
# `%!in%` <- Negate(`%in%`)
# K-M-O factor adequacy test
# * 0.00 to 0.49 unacceptable
# * 0.50 to 0.59 miserable
# * 0.60 to 0.69 mediocre
# * 0.70 to 0.79 middling
# * 0.80 to 0.89 meritorious
# => 0.90 to 1.00 marvelous
round(KMO(df_occupations.numeric[, KMO(df_occupations.numeric)$MSAi > 0.50])$MSA, 2)
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
cortest.bartlett(df_occupations.numeric)
# Determine Number of Factors to Extract
df_occupations.numeric %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1)
# Kaiser criterion suggests 7 factors are sufficient
# Scree plot
df_occupations.numeric %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df_occupations.numeric %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Parallel analysis suggests 8 factors are sufficient
# Very simple structure criterion (VSS)
df_occupations.numeric %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
# VSS criterion 1 suggests 2 factors are sufficient
# VSS criterion 2 suggests 3 factors are sufficient
# Velicer MAP criterion suggests 9 factors are sufficient
# Factor Analysis
n.facts <- 6
# Initially, we suppose that factors could be correlated to one another.
# Therefore, we apply an oblique rotation to the factors.
fit <- factanal(df_occupations.numeric, n.facts, rotation = 'promax')
print(fit, digits = 2, cutoff = 0.3, sort = T)
# Factors are somewhat correlated.
# => Orthogonal rotation does not seem appropriate.
# => Keep Oblique rotation factors
# Plot factors
fa.diagram(fit$loadings)
# Evaluation
# Do variables load to the factors sufficiently?
# |factor loading| > 0.4
fct_load.sufficient <- abs(fit$loadings) > 0.4
# Variables sufficiently load to each factor
colSums(fct_load.sufficient)
nrow(fct_load.sufficient)
# Variables that sufficiently load to each factor (%)
round(colSums(fct_load.sufficient)/nrow(fct_load.sufficient), 2)
# All factors have variables that sufficiently load to them
round(colSums(fct_load.sufficient)/nrow(fct_load.sufficient), 2) > 0
# Do all factors have at least three - or, better, four - or more variables loading onto them?
colSums(fct_load.sufficient) >= 3
colSums(fct_load.sufficient) >= 4
# Crossloadings: variables that load to more than one factor with loading values within 0.05 of one another
fit$loadings[,] %>%
as.matrix() %>%
as_tibble(rownames = 'Metric') %>%
mutate(Metric = factor(Metric)) -> df_loadings
# Max loading vs other loadings
df_loadings %>%
pivot_longer(#Convert to long data format
cols = starts_with('Factor')
, names_to = 'Factor'
, values_to = 'Loading'
) %>%
group_by(Metric) %>%
mutate(
Loading.Max = max(Loading) #Max loading per variable
, Loading.Diff.Abs = abs(Loading.Max - Loading) #Absolute difference
, Diff.Significant = ifelse(#Whether the difference is significant or not (i.e. <= 0.05)
Loading.Diff.Abs == 0
, yes = F #Loading.Diff.Abs == 0 <=> Max Loading (i.e. difference between max value and itself)
, no = Loading.Diff.Abs <= 0.05
)
) -> df_loadings.long
# Loadings Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading
)) +
geom_tile() +
scale_fill_gradient2(
low = "#FF0000"
, mid = "#FFFFCC"
, high = "#075AFF")
# Max Loadings Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading == Loading.Max
)) +
geom_tile()
# Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading.Diff.Abs
)) +
geom_tile() +
scale_fill_gradient2(
low = "#FF0000"
, mid = "#FFFFCC"
, high = "#075AFF")
# Significant Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Diff.Significant
)) +
geom_tile()
# Internal consistency test: Cronbach's Alpha
# Do the factors form a coherent group in and of themselves?
# Matching items to factors
# df_manually_assign <- read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSphzWoCxoNaiaJcQUWKCMqUAT041Q8UqUgM7rSzIwYZb7FhttKJwNgtrFf-r7EgzXHFom4UjLl2ltk/pub?gid=26093558&single=true&output=csv')
df_loadings.long %>%
right_join(
df_manually_assign
, by = c('Metric' = 'Subspecies')
) -> df_loadings.long
df_loadings.long %>%
filter(
Factor == Classification6_Factor
) -> df_loadings.long.factors
# Separate factors into individual data frames
lapply(
str_sort(unique(df_loadings.long.factors$Factor))
, function(factors){
df_loadings.long.factors %>%
filter(
Factor == factors
) %>%
pull(Metric) %>%
factor(.) %>%
return(.)
}) -> list_chr_loadings.long.factors
# Apply Alpha test to each subset of variables
lapply(
list_chr_loadings.long.factors
, function(factors){
df_occupations.numeric %>%
select(factors) -> df.temp #Select only the variables that match to each factor
if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
df.temp %>%
alpha(.) %>%
return(.)
}
else{
return(NA)
}
}) -> list_alpha
# Raw alpha score
lapply(
seq_along(list_alpha)
, function(index){
if(!is.na(list_alpha[index])){
list_alpha[[index]]$total$raw_alpha %>%
return(.)
}
else{
return(NA)
}
}) -> list_raw_alpha
# The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
lapply(
list_raw_alpha
, function(cronbach){
cronbach %>%
as_tibble(.) -> cronbach
colnames(cronbach) <- 'Cronbach_Alpha'
cronbach %>%
mutate(
Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
) %>%
return(.)
}) %>%
bind_rows(.) %>%
mutate(
Factor = paste0('Factor', row_number())
) -> df_raw_alpha
df_raw_alpha
# Conclusion:
# R, I, S, E, and C (Factors 1, 3, 4, 5, and 6) are internally consistent
# A (Factor 3) is not
# Factor 1 has 8 fields of knowledge loading to it.
# Factor 2 has 7 fields of knowledge loading to it.
# Factor 3 has 4 fields of knowledge loading to it.
# Factor 4 has 7 fields of knowledge loading to it.
# Factor 5 has 4 fields of knowledge loading to it.
# Factor 6 has 3 fields of knowledge loading to it.
df_loadings.long.factors %>%
group_by(Factor) %>%
tally(.) %>%
mutate(prct = round(n/sum(n),2))
# Manual EFA VII (Wikipedia): 5 factors (Humanities, Social Sciences, Natural Sciences, Formal Sciences, Applied Sciences) --------------------------------------------------------
# # Exploratory Factor Analysis (EFA)
# 'Not in' operator
# `%!in%` <- Negate(`%in%`)
# K-M-O factor adequacy test
# * 0.00 to 0.49 unacceptable
# * 0.50 to 0.59 miserable
# * 0.60 to 0.69 mediocre
# * 0.70 to 0.79 middling
# * 0.80 to 0.89 meritorious
# => 0.90 to 1.00 marvelous
round(KMO(df_occupations.numeric[, KMO(df_occupations.numeric)$MSAi > 0.50])$MSA, 2)
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
cortest.bartlett(df_occupations.numeric)
# Determine Number of Factors to Extract
df_occupations.numeric %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1)
# Kaiser criterion suggests 7 factors are sufficient
# Scree plot
df_occupations.numeric %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df_occupations.numeric %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Parallel analysis suggests 8 factors are sufficient
# Very simple structure criterion (VSS)
df_occupations.numeric %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
# VSS criterion 1 suggests 2 factors are sufficient
# VSS criterion 2 suggests 3 factors are sufficient
# Velicer MAP criterion suggests 9 factors are sufficient
# Factor Analysis
n.facts <- 5
# Initially, we suppose that factors could be correlated to one another.
# Therefore, we apply an oblique rotation to the factors.
fit <- factanal(df_occupations.numeric, n.facts, rotation = 'promax')
print(fit, digits = 2, cutoff = 0.3, sort = T)
# Factors are somewhat correlated.
# => Orthogonal rotation does not seem appropriate.
# => Keep Oblique rotation factors
# Plot factors
fa.diagram(fit$loadings)
# Evaluation
# Do variables load to the factors sufficiently?
# |factor loading| > 0.4
fct_load.sufficient <- abs(fit$loadings) > 0.4
# Variables sufficiently load to each factor
colSums(fct_load.sufficient)
nrow(fct_load.sufficient)
# Variables that sufficiently load to each factor (%)
round(colSums(fct_load.sufficient)/nrow(fct_load.sufficient), 2)
# All factors have variables that sufficiently load to them
round(colSums(fct_load.sufficient)/nrow(fct_load.sufficient), 2) > 0
# Do all factors have at least three - or, better, four - or more variables loading onto them?
colSums(fct_load.sufficient) >= 3
colSums(fct_load.sufficient) >= 4
# Crossloadings: variables that load to more than one factor with loading values within 0.05 of one another
fit$loadings[,] %>%
as.matrix() %>%
as_tibble(rownames = 'Metric') %>%
mutate(Metric = factor(Metric)) -> df_loadings
# Max loading vs other loadings
df_loadings %>%
pivot_longer(#Convert to long data format
cols = starts_with('Factor')
, names_to = 'Factor'
, values_to = 'Loading'
) %>%
group_by(Metric) %>%
mutate(
Loading.Max = max(Loading) #Max loading per variable
, Loading.Diff.Abs = abs(Loading.Max - Loading) #Absolute difference
, Diff.Significant = ifelse(#Whether the difference is significant or not (i.e. <= 0.05)
Loading.Diff.Abs == 0
, yes = F #Loading.Diff.Abs == 0 <=> Max Loading (i.e. difference between max value and itself)
, no = Loading.Diff.Abs <= 0.05
)
) -> df_loadings.long
# Loadings Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading
)) +
geom_tile() +
scale_fill_gradient2(
low = "#FF0000"
, mid = "#FFFFCC"
, high = "#075AFF")
# Max Loadings Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading == Loading.Max
)) +
geom_tile()
# Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Loading.Diff.Abs
)) +
geom_tile() +
scale_fill_gradient2(
low = "#FF0000"
, mid = "#FFFFCC"
, high = "#075AFF")
# Significant Loadings Difference Heatmap
df_loadings.long %>%
ggplot(aes(
x = fct_inorder(Factor)
, y = fct_rev(fct_inorder(Metric))
, fill = Diff.Significant
)) +
geom_tile()
# Internal consistency test: Cronbach's Alpha
# Do the factors form a coherent group in and of themselves?
# Matching items to factors
# df_manually_assign <- read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSphzWoCxoNaiaJcQUWKCMqUAT041Q8UqUgM7rSzIwYZb7FhttKJwNgtrFf-r7EgzXHFom4UjLl2ltk/pub?gid=26093558&single=true&output=csv')
df_loadings.long %>%
right_join(
df_manually_assign
, by = c('Metric' = 'Subspecies')
) -> df_loadings.long
df_loadings.long %>%
filter(
Factor == Classification7_Factor
) -> df_loadings.long.factors
# Separate factors into individual data frames
lapply(
str_sort(unique(df_loadings.long.factors$Factor))
, function(factors){
df_loadings.long.factors %>%
filter(
Factor == factors
) %>%
pull(Metric) %>%
factor(.) %>%
return(.)
}) -> list_chr_loadings.long.factors
# Apply Alpha test to each subset of variables
lapply(
list_chr_loadings.long.factors
, function(factors){
df_occupations.numeric %>%
select(factors) -> df.temp #Select only the variables that match to each factor
if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
df.temp %>%
alpha(.) %>%
return(.)
}
else{
return(NA)
}
}) -> list_alpha
# Raw alpha score
lapply(
seq_along(list_alpha)
, function(index){
if(!is.na(list_alpha[index])){
list_alpha[[index]]$total$raw_alpha %>%
return(.)
}
else{
return(NA)
}
}) -> list_raw_alpha
# The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
lapply(
list_raw_alpha
, function(cronbach){
cronbach %>%
as_tibble(.) -> cronbach
colnames(cronbach) <- 'Cronbach_Alpha'
cronbach %>%
mutate(
Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
) %>%
return(.)
}) %>%
bind_rows(.) %>%
mutate(
Factor = paste0('Factor', row_number())
) -> df_raw_alpha
df_raw_alpha
# Conclusion:
# All sciences (Factors 2, 3, 4, and 5) are internally consistent
# Humanities (Factor 1) are not consistent
# Factor 1 has 5 fields of knowledge loading to it.
# Factor 2 has 8 fields of knowledge loading to it.
# Factor 3 has only 3 fields of knowledge loading to it.
# Factor 4 has only 2 fields of knowledge loading to it.
# Factor 5 has 15 fields of knowledge loading to it.
df_loadings.long.factors %>%
group_by(Factor) %>%
tally(.) %>%
mutate(prct = round(n/sum(n),2))
