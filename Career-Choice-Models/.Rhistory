group_by(label) %>%
tally(.) %>%
filter(n > 1) %>%
pull(label) %>%
as.character(.) -> fields_knowledge
lapply(
fields_knowledge
, function(knowledge){
df_occupations.numeric %>%
select(where(
function(x){str_detect(attributes(x)$label, knowledge)}
)) -> df.temp
print(round(KMO(df.temp[, KMO(df.temp)$MSAi > 0.50])$MSA, 2))
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
print(cortest.bartlett(df.temp))
# Determine Number of Factors to Extract
df.temp %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1) -> kaiser
print(paste('Kaiser criterion suggests', kaiser,'factors are sufficient'))
# Scree plot
df.temp %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df.temp %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Parallel analysis suggests 8 factors are sufficient
# Very simple structure criterion (VSS)
df.temp %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
# VSS criterion 1 suggests 2 factors are sufficient
# VSS criterion 2 suggests 3 factors are sufficient
# Velicer MAP criterion suggests 9 factors are sufficient
# Factor Analysis
n.facts <- kaiser
# Initially, we suppose that factors could be correlated to one another.
# Therefore, we apply an oblique rotation to the factors.
fit <- factanal(df.temp, n.facts, rotation = 'promax')
print(fit, digits = 2, cutoff = 0.3, sort = T)
# Factors are somewhat uncorrelated to one another.
# => Orthogonal rotation could be appropriate.
# => Try Oblique rotation factors first
# Plot factors
fa.diagram(fit$loadings)
# On first sight, all factors seem relevant.
# Crossloadings: variables that load to more than one factor with loading values within 0.05 of one another
fit$loadings[,] %>%
as.matrix() %>%
as_tibble(rownames = 'Metric') %>%
mutate(Metric = factor(Metric)) -> df_loadings
# Max loading vs other loadings
df_loadings %>%
pivot_longer(#Convert to long data format
cols = starts_with('Factor')
, names_to = 'Factor'
, values_to = 'Loading'
) %>%
group_by(Metric) %>%
mutate(
Loading.Max = max(Loading) #Max loading per variable
, Loading.Diff.Abs = abs(Loading.Max - Loading) #Absolute difference
, Diff.Significant = ifelse(#Whether the difference is significant or not (i.e. <= 0.05)
Loading.Diff.Abs == 0
, yes = F #Loading.Diff.Abs == 0 <=> Max Loading (i.e. difference between max value and itself)
, no = Loading.Diff.Abs <= 0.05
)
) -> df_loadings.long
df_loadings.long %>%
filter(
Loading == Loading.Max
) -> df_loadings.long.factors
# Separate factors into individual data frames
lapply(
str_sort(str_sort(unique(df_loadings.long.factors$Factor)))
, function(factors){
df_loadings.long.factors %>%
filter(
Factor == factors
) %>%
pull(Metric) %>%
factor(.) %>%
return(.)
}) -> list_chr_loadings.long.factors
print(list_chr_loadings.long.factors)
# # Apply Alpha test to each subset of variables
# lapply(
#   list_chr_loadings.long.factors
#   , function(factors){
#
#     df.temp %>%
#       select(factors) -> df.temp.temp #Select only the variables that match to each factor
#
#     if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
#
#       df.temp.temp %>%
#         alpha(.) %>%
#         return(.)
#
#     }
#     else{
#
#       return(NA)
#
#     }
#
#   }) -> list_alpha
#
# # Raw alpha score
# lapply(
#   seq_along(list_alpha)
#   , function(index){
#
#     if(!is.na(list_alpha[index])){
#
#       list_alpha[[index]]$total$raw_alpha %>%
#         return(.)
#
#     }
#     else{
#
#       return(NA)
#
#     }
#
#   }) -> list_raw_alpha
#
# # The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
# lapply(
#   list_raw_alpha
#   , function(cronbach){
#
#     cronbach %>%
#       as_tibble(.) -> cronbach
#
#     colnames(cronbach) <- 'Cronbach_Alpha'
#
#     cronbach %>%
#       mutate(
#         Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
#       ) %>%
#       return(.)
#
#   }) %>%
#   bind_rows(.) %>%
#   mutate(
#     Factor = paste0('Factor', row_number())
#   ) -> df_raw_alpha
#
# return(df_raw_alpha)
}) -> dsds
, function(knowledge){
df_occupations.numeric %>%
select(where(
function(x){str_detect(attributes(x)$label, knowledge)}
)) -> df.temp
print(round(KMO(df.temp[, KMO(df.temp)$MSAi > 0.50])$MSA, 2))
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
print(cortest.bartlett(df.temp))
# Determine Number of Factors to Extract
df.temp %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1) -> kaiser
print(paste('Kaiser criterion suggests', kaiser,'factors are sufficient'))
# Scree plot
df.temp %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df.temp %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Parallel analysis suggests 8 factors are sufficient
# Very simple structure criterion (VSS)
df.temp %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
# VSS criterion 1 suggests 2 factors are sufficient
# VSS criterion 2 suggests 3 factors are sufficient
# Velicer MAP criterion suggests 9 factors are sufficient
# Factor Analysis
n.facts <- kaiser
# Initially, we suppose that factors could be correlated to one another.
# Therefore, we apply an oblique rotation to the factors.
fit <- factanal(df.temp, n.facts, rotation = 'promax')
print(fit, digits = 2, cutoff = 0.3, sort = T)
# Factors are somewhat uncorrelated to one another.
# => Orthogonal rotation could be appropriate.
# => Try Oblique rotation factors first
# Plot factors
fa.diagram(fit$loadings)
# On first sight, all factors seem relevant.
# Crossloadings: variables that load to more than one factor with loading values within 0.05 of one another
fit$loadings[,] %>%
as.matrix() %>%
as_tibble(rownames = 'Metric') %>%
mutate(Metric = factor(Metric)) -> df_loadings
return(df_loadings)
# # Max loading vs other loadings
# df_loadings %>%
#   pivot_longer(#Convert to long data format
#     cols = starts_with('Factor')
#     , names_to = 'Factor'
#     , values_to = 'Loading'
#   ) %>%
#   group_by(Metric) %>%
#   mutate(
#     Loading.Max = max(Loading) #Max loading per variable
#     , Loading.Diff.Abs = abs(Loading.Max - Loading) #Absolute difference
#     , Diff.Significant = ifelse(#Whether the difference is significant or not (i.e. <= 0.05)
#       Loading.Diff.Abs == 0
#       , yes = F #Loading.Diff.Abs == 0 <=> Max Loading (i.e. difference between max value and itself)
#       , no = Loading.Diff.Abs <= 0.05
#     )
#   ) -> df_loadings.long
#
# df_loadings.long %>%
#   filter(
#     Loading == Loading.Max
#   ) -> df_loadings.long.factors
#
# # Separate factors into individual data frames
# lapply(
#   str_sort(str_sort(unique(df_loadings.long.factors$Factor)))
#   , function(factors){
#
#     df_loadings.long.factors %>%
#       filter(
#         Factor == factors
#       ) %>%
#       pull(Metric) %>%
#       factor(.) %>%
#       return(.)
#
#   }) -> list_chr_loadings.long.factors
#
# print(list_chr_loadings.long.factors)
# # # Apply Alpha test to each subset of variables
# # lapply(
# #   list_chr_loadings.long.factors
# #   , function(factors){
# #
# #     df.temp %>%
# #       select(factors) -> df.temp.temp #Select only the variables that match to each factor
# #
# #     if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
# #
# #       df.temp.temp %>%
# #         alpha(.) %>%
# #         return(.)
# #
# #     }
# #     else{
# #
# #       return(NA)
# #
# #     }
# #
# #   }) -> list_alpha
# #
# # # Raw alpha score
# # lapply(
# #   seq_along(list_alpha)
# #   , function(index){
# #
# #     if(!is.na(list_alpha[index])){
# #
# #       list_alpha[[index]]$total$raw_alpha %>%
# #         return(.)
# #
# #     }
# #     else{
# #
# #       return(NA)
# #
# #     }
# #
# #   }) -> list_raw_alpha
# #
# # # The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
# # lapply(
# #   list_raw_alpha
# #   , function(cronbach){
# #
# #     cronbach %>%
# #       as_tibble(.) -> cronbach
# #
# #     colnames(cronbach) <- 'Cronbach_Alpha'
# #
# #     cronbach %>%
# #       mutate(
# #         Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
# #       ) %>%
# #       return(.)
# #
# #   }) %>%
# #   bind_rows(.) %>%
# #   mutate(
# #     Factor = paste0('Factor', row_number())
# #   ) -> df_raw_alpha
# #
# # return(df_raw_alpha)
#
}) -> dsds
# FACTOR EACH LABEL SEPARATELY (EPISTEMOLOGICAL) -------------------------------------
df_occupations.numeric %>%
labelled::generate_dictionary(.) %>%
select(label) %>%
group_by(label) %>%
tally(.) %>%
filter(n > 1) %>%
pull(label) %>%
as.character(.) -> fields_knowledge
lapply(
fields_knowledge
, function(knowledge){
df_occupations.numeric %>%
select(where(
function(x){str_detect(attributes(x)$label, knowledge)}
)) -> df.temp
print(round(KMO(df.temp[, KMO(df.temp)$MSAi > 0.50])$MSA, 2))
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
print(cortest.bartlett(df.temp))
# Determine Number of Factors to Extract
df.temp %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1) -> kaiser
print(paste('Kaiser criterion suggests', kaiser,'factors are sufficient'))
# Scree plot
df.temp %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df.temp %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Parallel analysis suggests 8 factors are sufficient
# Very simple structure criterion (VSS)
df.temp %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
# VSS criterion 1 suggests 2 factors are sufficient
# VSS criterion 2 suggests 3 factors are sufficient
# Velicer MAP criterion suggests 9 factors are sufficient
# Factor Analysis
n.facts <- kaiser
# Initially, we suppose that factors could be correlated to one another.
# Therefore, we apply an oblique rotation to the factors.
fit <- factanal(df.temp, n.facts, rotation = 'promax')
print(fit, digits = 2, cutoff = 0.3, sort = T)
# Factors are somewhat uncorrelated to one another.
# => Orthogonal rotation could be appropriate.
# => Try Oblique rotation factors first
# Plot factors
fa.diagram(fit$loadings)
# On first sight, all factors seem relevant.
# Crossloadings: variables that load to more than one factor with loading values within 0.05 of one another
fit$loadings[,] %>%
as.matrix() %>%
as_tibble(rownames = 'Metric') %>%
mutate(Metric = factor(Metric)) -> df_loadings
return(df_loadings)
# # Max loading vs other loadings
# df_loadings %>%
#   pivot_longer(#Convert to long data format
#     cols = starts_with('Factor')
#     , names_to = 'Factor'
#     , values_to = 'Loading'
#   ) %>%
#   group_by(Metric) %>%
#   mutate(
#     Loading.Max = max(Loading) #Max loading per variable
#     , Loading.Diff.Abs = abs(Loading.Max - Loading) #Absolute difference
#     , Diff.Significant = ifelse(#Whether the difference is significant or not (i.e. <= 0.05)
#       Loading.Diff.Abs == 0
#       , yes = F #Loading.Diff.Abs == 0 <=> Max Loading (i.e. difference between max value and itself)
#       , no = Loading.Diff.Abs <= 0.05
#     )
#   ) -> df_loadings.long
#
# df_loadings.long %>%
#   filter(
#     Loading == Loading.Max
#   ) -> df_loadings.long.factors
#
# # Separate factors into individual data frames
# lapply(
#   str_sort(str_sort(unique(df_loadings.long.factors$Factor)))
#   , function(factors){
#
#     df_loadings.long.factors %>%
#       filter(
#         Factor == factors
#       ) %>%
#       pull(Metric) %>%
#       factor(.) %>%
#       return(.)
#
#   }) -> list_chr_loadings.long.factors
#
# print(list_chr_loadings.long.factors)
# # # Apply Alpha test to each subset of variables
# # lapply(
# #   list_chr_loadings.long.factors
# #   , function(factors){
# #
# #     df.temp %>%
# #       select(factors) -> df.temp.temp #Select only the variables that match to each factor
# #
# #     if(length(factors) > 1){#By definition, internal consistency tests only apply to groups of more than one variable
# #
# #       df.temp.temp %>%
# #         alpha(.) %>%
# #         return(.)
# #
# #     }
# #     else{
# #
# #       return(NA)
# #
# #     }
# #
# #   }) -> list_alpha
# #
# # # Raw alpha score
# # lapply(
# #   seq_along(list_alpha)
# #   , function(index){
# #
# #     if(!is.na(list_alpha[index])){
# #
# #       list_alpha[[index]]$total$raw_alpha %>%
# #         return(.)
# #
# #     }
# #     else{
# #
# #       return(NA)
# #
# #     }
# #
# #   }) -> list_raw_alpha
# #
# # # The general rule of thumb is that a Cronbach's alpha of 0.70 and above is good
# # lapply(
# #   list_raw_alpha
# #   , function(cronbach){
# #
# #     cronbach %>%
# #       as_tibble(.) -> cronbach
# #
# #     colnames(cronbach) <- 'Cronbach_Alpha'
# #
# #     cronbach %>%
# #       mutate(
# #         Cronbach_Alpha.Good = Cronbach_Alpha >= 0.7
# #       ) %>%
# #       return(.)
# #
# #   }) %>%
# #   bind_rows(.) %>%
# #   mutate(
# #     Factor = paste0('Factor', row_number())
# #   ) -> df_raw_alpha
# #
# # return(df_raw_alpha)
#
}) -> dsds
dsds
# FACTOR EACH LABEL SEPARATELY (EPISTEMOLOGICAL) -------------------------------------
df_occupations.numeric %>%
labelled::generate_dictionary(.) %>%
select(label) %>%
group_by(label) %>%
tally(.) %>%
filter(n > 1) %>%
pull(label) %>%
as.character(.) -> fields_knowledge
fields_knowledge[1] -> knowledge
knowledge
df_occupations %>%
select(where(
function(x){str_detect(attributes(x)$label, knowledge)}
)) -> df_occupations.temp
# K-M-O factor adequacy test
# * 0.00 to 0.49 unacceptable
# * 0.50 to 0.59 miserable
# * 0.60 to 0.69 mediocre
# * 0.70 to 0.79 middling
# * 0.80 to 0.89 meritorious
# => 0.90 to 1.00 marvelous
round(KMO(df_occupations.temp[, KMO(df_occupations.temp)$MSAi > 0.50])$MSA, 2)
# Null hypothesis: the variables in the data set are essentially uncorrelated.
# Rejected the null hypothesis. Therefore, the data may be grouped into factors.
cortest.bartlett(df_occupations.temp)
# Determine Number of Factors to Extract
df_occupations.temp %>%
cor() %>%
eigen() -> ev
sum(ev$values >= 1)
# Scree plot
df_occupations.temp %>%
scree(pc = F) #Factor analysis => pc = False
# Parallel analysis
df_occupations.temp %>%
fa.parallel(fa = 'fa') -> pa_analysis
# Very simple structure criterion (VSS)
df_occupations.temp %>%
VSS(n = 2 * pa_analysis$nfact) %>%
summary()
